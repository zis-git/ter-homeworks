# Домашнее задание к занятию «Основы Terraform. Yandex Cloud»  Голоха Е.В.

### Цели задания

1. Создать свои ресурсы в облаке Yandex Cloud с помощью Terraform.
2. Освоить работу с переменными Terraform.


### Чек-лист готовности к домашнему заданию

1. Зарегистрирован аккаунт в Yandex Cloud. Использован промокод на грант.
2. Установлен инструмент Yandex CLI.
3. Исходный код для выполнения задания расположен в директории [**02/src**](https://github.com/netology-code/ter-homeworks/tree/main/02/src).


### Задание 0

1. Ознакомьтесь с [документацией к security-groups в Yandex Cloud](https://cloud.yandex.ru/docs/vpc/concepts/security-groups?from=int-console-help-center-or-nav). 
Этот функционал понадобится к следующей лекции.

------
### Внимание!! Обязательно предоставляем на проверку получившийся код в виде ссылки на ваш github-репозиторий!
------

### Задание 1
В качестве ответа всегда полностью прикладывайте ваш terraform-код в git.
Убедитесь что ваша версия **Terraform** ~>1.12.0

1. Изучите проект. В файле variables.tf объявлены переменные для Yandex provider.
2. Создайте сервисный аккаунт и ключ. [service_account_key_file](https://terraform-provider.yandexcloud.net).
4. Сгенерируйте новый или используйте свой текущий ssh-ключ. Запишите его открытую(public) часть в переменную **vms_ssh_public_root_key**.
5. Инициализируйте проект, выполните код. Исправьте намеренно допущенные синтаксические ошибки. Ищите внимательно, посимвольно. Ответьте, в чём заключается их суть.
6. Подключитесь к консоли ВМ через ssh и выполните команду ``` curl ifconfig.me```.
Примечание: К OS ubuntu "out of a box, те из коробки" необходимо подключаться под пользователем ubuntu: ```"ssh ubuntu@vm_ip_address"```. Предварительно убедитесь, что ваш ключ добавлен в ssh-агент: ```eval $(ssh-agent) && ssh-add``` Вы познакомитесь с тем как при создании ВМ создать своего пользователя в блоке metadata в следующей лекции.;
8. Ответьте, как в процессе обучения могут пригодиться параметры ```preemptible = true``` и ```core_fraction=5``` в параметрах ВМ.

В качестве решения приложите:

- скриншот ЛК Yandex Cloud с созданной ВМ, где видно внешний ip-адрес;
- скриншот консоли, curl должен отобразить тот же внешний ip-адрес;
- ответы на вопросы.


### Решение 1

Для подключения к Yandex Cloud я не хардкодил cloud_id, folder_id, зону и токен в провайдере.
Все значения вынесены в переменные (var.cloud_id, var.folder_id, var.default_zone).
Авторизация производится через service_account_key_file, путь к файлу ключа, а не через прямой OAuth-токен в коде.

variables.tf (фрагмент):

""
variable "cloud_id" {
  type        = string
  description = "https://cloud.yandex.ru/docs/resource-manager/operations/cloud/get-id"
}

variable "folder_id" {
  type        = string
  description = "https://cloud.yandex.ru/docs/resource-manager/operations/folder/get-id"
}

variable "default_zone" {
  type        = string
  default     = "ru-central1-a"
  description = "https://cloud.yandex.ru/docs/overview/concepts/geo-scope"
}

variable "default_cidr" {
  type        = list(string)
  default     = ["10.0.1.0/24"]
  description = "CIDR для подсети"
}

variable "vpc_name" {
  type        = string
  default     = "develop"
  description = "VPC network & subnet name"
}
""

terraform.tfvars
cloud_id  = "b1g3gq2pvbd3dnherq8j"
folder_id = "b1gio4d52dv3cbcba4ad"

начения cloud_id и folder_id хранятся отдельно в terraform.tfvars и не попадают в код ресурсов.




<img width="489" height="32" alt="Снимок экрана 2025-10-29 в 19 16 18" src="https://github.com/user-attachments/assets/62712e0f-a60f-4cd2-b14f-a4ff8ffec22d" />
<img width="656" height="183" alt="Снимок экрана 2025-10-29 в 19 40 49" src="https://github.com/user-attachments/assets/17b77754-3137-4eea-8ae9-737d59624d07" />


### Задание 2

1. Замените все хардкод-**значения** для ресурсов **yandex_compute_image** и **yandex_compute_instance** на **отдельные** переменные. К названиям переменных ВМ добавьте в начало префикс **vm_web_** .  Пример: **vm_web_name**.
2. Объявите нужные переменные в файле variables.tf, обязательно указывайте тип переменной. Заполните их **default** прежними значениями из main.tf. 
3. Проверьте terraform plan. Изменений быть не должно. 


### Решение 2

Все параметры для ресурсов yandex_compute_image и yandex_compute_instance вынесены в переменные.

Для веб-сервера используется префикс vm_web_
""

variable "vm_web_name" {
  type    = string
  default = "netology-develop-platform-web"
}
""
Конфигурация валидна:
<img width="471" height="49" alt="Снимок экрана 2025-10-29 в 19 29 31" src="https://github.com/user-attachments/assets/66400d0f-b81e-4690-be41-0ac70ce6a986" />

<img width="650" height="533" alt="Снимок экрана 2025-10-29 в 19 30 20" src="https://github.com/user-attachments/assets/676cd247-60a3-406a-add5-661ef94846b9" />

все значения вынесены в переменные, изменений нет.


### Задание 3

1. Создайте в корне проекта файл 'vms_platform.tf' . Перенесите в него все переменные первой ВМ.
2. Скопируйте блок ресурса и создайте с его помощью вторую ВМ в файле main.tf: **"netology-develop-platform-db"** ,  ```cores  = 2, memory = 2, core_fraction = 20```. Объявите её переменные с префиксом **vm_db_** в том же файле ('vms_platform.tf').  ВМ должна работать в зоне "ru-central1-b"
3. Примените изменения.

### Решение 3
В проект добавлен файл:
/home/zis/ter-homeworks/02/src/vms_platform.tf
В этот файл были вынесены переменные, описывающие обе виртуальные машины: web и db.
""
resource "yandex_compute_instance" "web" {
  count       = 2
  name        = "web-${count.index + 1}"
  platform_id = "standard-v1"

  resources {
    cores         = 2
    memory        = 2
    core_fraction = 20
  }

  boot_disk {
    initialize_params {
      image_id = data.yandex_compute_image.ubuntu.image_id
      type     = "network-hdd"
      size     = 10
    }
  }

  scheduling_policy {
    preemptible = true
  }

  network_interface {
    subnet_id          = yandex_vpc_subnet.develop.id
    nat                = true
    security_group_ids = [yandex_vpc_security_group.example.id]
  }

  metadata = local.metadata_common

  depends_on = [yandex_compute_instance.db]
}

""

Веб-серверы создаются без хардкода:
имена web-1, web-2 генерируются через "web-${count.index + 1}",
общая конфигурация вынесена в один ресурс с count = 2,
образ не захардкожен по ID, а выбирается через data "yandex_compute_image" "ubuntu" { family = "ubuntu-2004-lts" }.
При изменении count можно мгновенно получить другое количество веб-нод без копипасты ресурсов.

Для web-виртуалки используются переменные с префиксом vm_web_....
Для db-виртуалки используются переменные с префиксом vm_db_....
Значения default взяты из того, что раньше было захардкожено в main.tf




### Задание 4

1. Объявите в файле outputs.tf **один** output , содержащий: instance_name, external_ip, fqdn для каждой из ВМ в удобном лично для вас формате.(без хардкода!!!)
2. Примените изменения.

В качестве решения приложите вывод значений ip-адресов команды ```terraform output```.


### Решение 4

Машины db-main и db-replica создаются через один ресурс с for_each.
Их параметры (имя, CPU, RAM, размер диска) я описываю в переменной var.each_vm, а не хардкожу прямо внутри ресурса.
Это позволяет добавлять новые роли БД просто дописав объект в each_vm, без копирования HCL-кода.

Я завёл в variables.tf переменную each_vm, которая описывает параметры каждой ВМ как объект:
""
variable "each_vm" {
  type = list(object({
    vm_name     = string
    cpu         = number
    ram         = number
    disk_volume = number
  }))

  default = [
    { vm_name = "main",    cpu = 2, ram = 2, disk_volume = 10 },
    { vm_name = "replica", cpu = 2, ram = 4, disk_volume = 12 }
  ]
}

""
А потом в for_each-vm.tf я создаю ресурс одной конструкцией:


""
resource "yandex_compute_instance" "db" {
  for_each = { for vm in var.each_vm : vm.vm_name => vm }

  name        = "db-${each.key}"
  platform_id = "standard-v1"

  resources {
    cores         = each.value.cpu
    memory        = each.value.ram
    core_fraction = 20
  }

  boot_disk {
    initialize_params {
      image_id = data.yandex_compute_image.ubuntu.image_id
      size     = each.value.disk_volume
      type     = "network-hdd"
    }
  }

  scheduling_policy {
    preemptible = true
  }

  network_interface {
    subnet_id          = yandex_vpc_subnet.develop.id
    nat                = true
    security_group_ids = [yandex_vpc_security_group.example.id]
  }

  metadata = local.metadata_common
}
""


### Задание 5

1. В файле locals.tf опишите в **одном** local-блоке имя каждой ВМ, используйте интерполяцию ${..} с НЕСКОЛЬКИМИ переменными по примеру из лекции.
2. Замените переменные внутри ресурса ВМ на созданные вами local-переменные.
3. Примените изменения.

### Решение 5
Как у меня сделано :
Файл disk_vm.tf

""
resource "yandex_compute_disk" "storage_disks" {
  count = 3
  name  = "storage-disk-${count.index + 1}"
  size  = 1
  type  = "network-hdd"
  zone  = var.default_zone
}

resource "yandex_compute_instance" "storage" {
  name        = "storage"
  platform_id = "standard-v1"

  resources {
    cores         = 2
    memory        = 2
    core_fraction = 20
  }

  boot_disk {
    initialize_params {
      image_id = data.yandex_compute_image.ubuntu.image_id
      type     = "network-hdd"
      size     = 10
    }
  }

  # вот самое важное - dynamic
  dynamic "secondary_disk" {
    for_each = yandex_compute_disk.storage_disks
    content {
      disk_id = secondary_disk.value.id
    }
  }

  scheduling_policy {
    preemptible = true
  }

  network_interface {
    subnet_id          = yandex_vpc_subnet.develop.id
    nat                = true
    security_group_ids = [yandex_vpc_security_group.example.id]
  }

  metadata = local.metadata_common
}
""
Дополнительные диски не захардкожены и не перечисляются вручную.
Диски сначала создаются ресурсом yandex_compute_disk.storage_disks с count = 3, а потом подключаются к ВМ storage через dynamic "secondary_disk" и цикл for_each.
Если мне нужно 5 дисков вместо 3 — я меняю только count = 5 в одном месте, сам код ресурса storage не меняется.
Это устраняет хардкод конфигурации дисков.


### Задание 6

1. Вместо использования трёх переменных  ".._cores",".._memory",".._core_fraction" в блоке  resources {...}, объедините их в единую map-переменную **vms_resources** и  внутри неё конфиги обеих ВМ в виде вложенного map(object).  
   ```
   пример из terraform.tfvars:
   vms_resources = {
     web={
       cores=2
       memory=2
       core_fraction=5
       hdd_size=10
       hdd_type="network-hdd"
       ...
     },
     db= {
       cores=2
       memory=4
       core_fraction=20
       hdd_size=10
       hdd_type="network-ssd"
       ...
     }
   }
   ```
3. Создайте и используйте отдельную map(object) переменную для блока metadata, она должна быть общая для всех ваших ВМ.
   ```
   пример из terraform.tfvars:
   metadata = {
     serial-port-enable = 1
     ssh-keys           = "ubuntu:ssh-ed25519 AAAAC..."
   }
   ```  
  
5. Найдите и закоментируйте все, более не используемые переменные проекта.
6. Проверьте terraform plan. Изменений быть не должно.

------

### Решение 6

Как сделано уменя:
Я вынес общее в locals.tf:

""
locals {
  ssh_public_key = chomp(file("/home/zis/.ssh/id_ed25519.pub"))

  metadata_common = {
    serial-port-enable = 1
    ssh-keys           = "ubuntu:${local.ssh_public_key}"
  }
}
""

А потом во всех VM делаю:
metadata = local.metadata_common

Метаданные (включая SSH-ключ для пользователя ubuntu) не хранятся в каждой виртуальной машине жёстко.
Я вынес содержимое в общий local.metadata_common и подключаю его одной строкой metadata = local.metadata_common.
Путь к ключу также берётся функцией file() — это убирает ручной копипаст открытого ключа в каждый ресурс.
