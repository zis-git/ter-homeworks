# Домашнее задание к занятию «Управляющие конструкции в коде Terraform» Голоха Е.В.

### Цели задания

### Задание 1

1. Изучите проект.
2. Инициализируйте проект, выполните код. 


Приложите скриншот входящих правил «Группы безопасности» в ЛК Yandex Cloud .

### Решение 1

Создана VPC-сеть develop, подсеть develop в зоне ru-central1-a и группа безопасности example_dynamic с динамическими правилами ingress/egress.
Настроен провайдер Yandex Cloud через сервисный аккаунт (authorized_key.json).

Инициализация Terraform:

<img width="677" height="452" alt="Снимок экрана 2025-10-29 в 09 15 04" src="https://github.com/user-attachments/assets/ff951802-1ddc-459f-9034-f1858cce673a" />

Проверка конфигурации:
<img width="465" height="59" alt="Снимок экрана 2025-10-29 в 09 19 44" src="https://github.com/user-attachments/assets/f85a42ee-c49d-4fa7-b511-e64176ce2149" />

Создание сети и подсети:
<img width="542" height="116" alt="Снимок экрана 2025-10-29 в 09 13 42" src="https://github.com/user-attachments/assets/2d89510a-934d-4d55-a5c8-65cf203304e6" />

<img width="913" height="466" alt="Снимок экрана 2025-10-29 в 09 28 33" src="https://github.com/user-attachments/assets/d5dffa13-b303-46a1-9195-65ebd1c3d335" />

Проверка ресурсов через yc CLI:

yc vpc network list

yc vpc subnet list

<img width="684" height="504" alt="Снимок экрана 2025-10-29 в 09 07 19" src="https://github.com/user-attachments/assets/aa21460e-6ca7-4973-9b64-36fa3e3e0c03" />

Результат:
Сеть и подсеть успешно созданы, группа безопасности активна и разрешает входящие SSH (22), HTTP (80), HTTPS (443).


### Задание 2

1. Создайте файл count-vm.tf. Опишите в нём создание двух **одинаковых** ВМ  web-1 и web-2 (не web-0 и web-1) с минимальными параметрами, используя мета-аргумент **count loop**. Назначьте ВМ созданную в первом задании группу безопасности.(как это сделать узнайте в документации провайдера yandex/compute_instance )
2. Создайте файл for_each-vm.tf. Опишите в нём создание двух ВМ для баз данных с именами "main" и "replica" **разных** по cpu/ram/disk_volume , используя мета-аргумент **for_each loop**. Используйте для обеих ВМ одну общую переменную типа:
```
variable "each_vm" {
  type = list(object({  vm_name=string, cpu=number, ram=number, disk_volume=number }))
}
```  
При желании внесите в переменную все возможные параметры.
4. ВМ из пункта 2.1 должны создаваться после создания ВМ из пункта 2.2.
5. Используйте функцию file в local-переменной для считывания ключа ~/.ssh/id_rsa.pub и его последующего использования в блоке metadata, взятому из ДЗ 2.
6. Инициализируйте проект, выполните код.

------
### Решение 2

count — создание двух web-серверов
count-vm.tf

<img width="657" height="111" alt="Снимок экрана 2025-10-29 в 09 40 06" src="https://github.com/user-attachments/assets/046af671-fb15-4cd0-9caa-3b66d807895d" />

Параметры:

count = 2

name = "web-${count.index + 1}"

cores = 2, memory = 2, core_fraction = 20

boot_disk.size = 10

preemptible = true

Проверка валидности:

<img width="604" height="340" alt="Снимок экрана 2025-10-29 в 09 41 54" src="https://github.com/user-attachments/assets/2d550901-46bc-41d6-a9de-a57cef66c744" />

Применение:

<img width="668" height="48" alt="Снимок экрана 2025-10-29 в 09 48 05" src="https://github.com/user-attachments/assets/a0ce7d69-000f-4dea-bffa-7be8b351e8e9" />


for_each — создание двух БД main и replica

Код:

Файл for_each-vm.tf использует переменную each_vm из variables.tf.

<img width="666" height="341" alt="Снимок экрана 2025-10-29 в 09 49 55" src="https://github.com/user-attachments/assets/8630b77c-4e68-4ddd-9d8f-8de7503f3df6" />

Проверка : 
<img width="674" height="54" alt="Снимок экрана 2025-10-29 в 09 53 39" src="https://github.com/user-attachments/assets/8b30808e-22bd-4dbc-aaa7-34efc7d79ab4" />



### Задание 3

1. Создайте 3 одинаковых виртуальных диска размером 1 Гб с помощью ресурса yandex_compute_disk и мета-аргумента count в файле **disk_vm.tf** .
2. Создайте в том же файле **одиночную**(использовать count или for_each запрещено из-за задания №4) ВМ c именем "storage"  . Используйте блок **dynamic secondary_disk{..}** и мета-аргумент for_each для подключения созданных вами дополнительных дисков.

------
### Решение 3
Код:
Файл disk_vm.tf
Создаётся три диска (count = 3) и одна ВМ storage, подключающая их через dynamic "secondary_disk".

Применение:
<img width="660" height="387" alt="Снимок экрана 2025-10-29 в 09 57 56" src="https://github.com/user-attachments/assets/661b6525-d1e5-4e34-8903-38f92f043123" />

ВМ storage успешно создана, к ней подключено 3 дополнительных диска по 1 ГБ каждый.

### Задание 4

1. В файле ansible.tf создайте inventory-файл для ansible.
Используйте функцию tepmplatefile и файл-шаблон для создания ansible inventory-файла из лекции.
Готовый код возьмите из демонстрации к лекции [**demonstration2**](https://github.com/netology-code/ter-homeworks/tree/main/03/demo).
Передайте в него в качестве переменных группы виртуальных машин из задания 2.1, 2.2 и 3.2, т. е. 5 ВМ.
2. Инвентарь должен содержать 3 группы и быть динамическим, т. е. обработать как группу из 2-х ВМ, так и 999 ВМ.
3. Добавьте в инвентарь переменную  [**fqdn**](https://cloud.yandex.ru/docs/compute/concepts/network#hostname).
``` 
[webservers]
web-1 ansible_host=<внешний ip-адрес> fqdn=<полное доменное имя виртуальной машины>
web-2 ansible_host=<внешний ip-адрес> fqdn=<полное доменное имя виртуальной машины>

[databases]
main ansible_host=<внешний ip-адрес> fqdn=<полное доменное имя виртуальной машины>
replica ansible_host<внешний ip-адрес> fqdn=<полное доменное имя виртуальной машины>

[storage]
storage ansible_host=<внешний ip-адрес> fqdn=<полное доменное имя виртуальной машины>
```
Пример fqdn: ```web1.ru-central1.internal```(в случае указания переменной hostname(не путать с переменной name)); ```fhm8k1oojmm5lie8i22a.auto.internal```(в случае отсутвия перменной hostname - автоматическая генерация имени,  зона изменяется на auto). нужную вам переменную найдите в документации провайдера или terraform console.
4. Выполните код. Приложите скриншот получившегося файла. 

Для общего зачёта создайте в вашем GitHub-репозитории новую ветку terraform-03. Закоммитьте в эту ветку свой финальный код проекта, пришлите ссылку на коммит.   
**Удалите все созданные ресурсы**.

------
### Решение 4
Создан шаблон ansible.tftpl и файл ansible.tf, в котором используется templatefile() для генерации ansible_inventory.ini.

Инициализация:

<img width="676" height="523" alt="Снимок экрана 2025-10-29 в 10 02 24" src="https://github.com/user-attachments/assets/4fee0918-2a0b-4330-8e77-d22278146757" />

Применение шаблона:
<img width="676" height="480" alt="Снимок экрана 2025-10-29 в 10 09 24" src="https://github.com/user-attachments/assets/cc6655e5-60db-494a-af14-3cc301a24de6" />

Содержимое итогового файла:
<img width="686" height="186" alt="Снимок экрана 2025-10-29 в 10 11 10" src="https://github.com/user-attachments/assets/aa377374-9e60-4982-987e-819f9dff7f3d" />


""
[webservers]
web-1 ansible_host=158.160.122.203 fqdn=fhm05ejes0bilmjt6b5m.auto.internal
web-2 ansible_host=158.160.115.46 fqdn=fhmrhbr6d9ppdmlboko3.auto.internal

[databases]
db-main ansible_host=158.160.120.129 fqdn=fhmhg8bjuc4ff8vkcllt9.auto.internal
db-replica ansible_host=158.160.97.63 fqdn=fhmtt62un56dmtsjafgd.auto.internal

[storage]
storage ansible_host=62.84.126.89 fqdn=fhmr3b39l4mhhvvfdk8c.auto.internal
""

Уничтожение инфраструктуры
Первое удаление (основные ресурсы):
<img width="676" height="204" alt="Снимок экрана 2025-10-29 в 10 29 41" src="https://github.com/user-attachments/assets/0e4b91f7-f651-4662-b967-44ae1dca55ef" />

Повторное выполнение (проверка):
<img width="668" height="254" alt="Снимок экрана 2025-10-29 в 10 33 19" src="https://github.com/user-attachments/assets/4f89665c-f653-4ebb-9630-f511a1b5d969" />


Дополнения по заданиям:
Структура каталога 03/src

03/src/
├─ providers.tf
├─ variables.tf
├─ locals.tf
├─ main.tf
├─ security.tf
├─ for_each-vm.tf
├─ count-vm.tf
├─ disk_vm.tf
├─ ansible.tftpl
├─ ansible.tf
├─ outputs.tf
└─ terraform.tfvars


Файл providers.tf

""
terraform {
  required_version = "~>1.12.0"

  required_providers {
    yandex = {
      source = "yandex-cloud/yandex"
    }
    local = {
      source = "hashicorp/local"
    }
  }
}

provider "yandex" {
  # Авторизация сервисным ключом (без OAuth в коде)
  service_account_key_file = file(var.service_account_key_path)

  cloud_id  = var.cloud_id
  folder_id = var.folder_id
  zone      = var.default_zone
}
""

Файл variables.tf

""
### Cloud / auth
variable "cloud_id"  { type = string, description = "YC cloud id"  }
variable "folder_id" { type = string, description = "YC folder id" }

variable "default_zone" {
  type        = string
  default     = "ru-central1-a"
  description = "Default Compute zone"
}

variable "default_cidr" {
  type        = list(string)
  default     = ["10.0.1.0/24"]
  description = "VPC subnet CIDR list"
}

variable "vpc_name" {
  type        = string
  default     = "develop"
  description = "VPC network & subnet name"
}

# Пути вынесены в переменные (без хардкода в ресурсах)
variable "service_account_key_path" {
  type        = string
  default     = "/home/zis/.authorized_key.json"
  description = "Path to service account key JSON"
}

variable "ssh_public_key_path" {
  type        = string
  default     = "/home/zis/.ssh/id_ed25519.pub"
  description = "Path to user's SSH public key"
}

# Карта ресурсов по ролям (web/db) — убирает хардкод CPU/RAM/CoreFraction
variable "vm_resources_map" {
  description = "VM resources by role"
  type = map(object({
    cores         = number
    memory        = number
    core_fraction = number
  }))
  default = {
    web = { cores = 2, memory = 2, core_fraction = 20 }
    db  = { cores = 2, memory = 4, core_fraction = 20 }
  }
}

# Перечень ВМ БД с разной конфигурацией — for_each
variable "each_vm" {
  type = list(object({
    vm_name     = string
    cpu         = number
    ram         = number
    disk_volume = number
  }))
  default = [
    { vm_name = "main",    cpu = 2, ram = 2, disk_volume = 10 },
    { vm_name = "replica", cpu = 2, ram = 4, disk_volume = 12 }
  ]
}

""

Файл locals.tf

""
locals {
  # Читаем публичный ключ с диска — без вставки ключей в код
  ssh_public_key = chomp(file(var.ssh_public_key_path))

  # Общий metadata для всех ВМ
  metadata_common = {
    serial-port-enable = 1
    ssh-keys           = "ubuntu:${local.ssh_public_key}"
  }

  # Единые метки
  common_labels = {
    project     = "netology"
    environment = "develop"
  }
}
""

Файл main.tf — сеть и подсеть

""resource "yandex_vpc_network" "develop" {
  name   = var.vpc_name
  labels = local.common_labels
}

resource "yandex_vpc_subnet" "develop" {
  name           = var.vpc_name
  zone           = var.default_zone
  network_id     = yandex_vpc_network.develop.id
  v4_cidr_blocks = var.default_cidr
  labels         = local.common_labels
}

""

security.tf — динамический SG

"variable "security_group_ingress" {
  description = "Ingress rules"
  type = list(object({
    protocol       = string
    description    = string
    v4_cidr_blocks = list(string)
    port           = optional(number)
    from_port      = optional(number)
    to_port        = optional(number)
  }))
  default = [
    { protocol = "TCP", description = "ssh",   v4_cidr_blocks = ["0.0.0.0/0"], port = 22  },
    { protocol = "TCP", description = "http",  v4_cidr_blocks = ["0.0.0.0/0"], port = 80  },
    { protocol = "TCP", description = "https", v4_cidr_blocks = ["0.0.0.0/0"], port = 443 },
  ]
}

variable "security_group_egress" {
  description = "Egress rules"
  type = list(object({
    protocol       = string
    description    = string
    v4_cidr_blocks = list(string)
    port           = optional(number)
    from_port      = optional(number)
    to_port        = optional(number)
  }))
  default = [
    { protocol = "TCP", description = "allow all egress", v4_cidr_blocks = ["0.0.0.0/0"], from_port = 0, to_port = 65365 }
  ]
}

resource "yandex_vpc_security_group" "example" {
  name       = "example_dynamic"
  network_id = yandex_vpc_network.develop.id
  folder_id  = var.folder_id
  labels     = local.common_labels

  dynamic "ingress" {
    for_each = var.security_group_ingress
    content {
      protocol       = lookup(ingress.value, "protocol", null)
      description    = lookup(ingress.value, "description", null)
      port           = lookup(ingress.value, "port", null)
      from_port      = lookup(ingress.value, "from_port", null)
      to_port        = lookup(ingress.value, "to_port", null)
      v4_cidr_blocks = lookup(ingress.value, "v4_cidr_blocks", null)
    }
  }

  dynamic "egress" {
    for_each = var.security_group_egress
    content {
      protocol       = lookup(egress.value, "protocol", null)
      description    = lookup(egress.value, "description", null)
      port           = lookup(egress.value, "port", null)
      from_port      = lookup(egress.value, "from_port", null)
      to_port        = lookup(egress.value, "to_port", null)
      v4_cidr_blocks = lookup(egress.value, "v4_cidr_blocks", null)
    }
  }
}


""
Файл 
for_each-vm.tf — БД (for_each)

""
data "yandex_compute_image" "ubuntu" {
  family = "ubuntu-2004-lts"
}

resource "yandex_compute_instance" "db" {
  for_each    = { for vm in var.each_vm : vm.vm_name => vm }
  name        = "db-${each.key}"
  platform_id = "standard-v1"
  labels      = local.common_labels

  resources {
    # cpu/ram — из each_vm
    cores         = each.value.cpu
    memory        = each.value.ram
    # доля CPU — из карты ролей (db)
    core_fraction = var.vm_resources_map["db"].core_fraction
  }

  boot_disk {
    initialize_params {
      image_id = data.yandex_compute_image.ubuntu.image_id
      size     = each.value.disk_volume
      type     = "network-hdd"
    }
  }

  scheduling_policy { preemptible = true }

  network_interface {
    subnet_id          = yandex_vpc_subnet.develop.id
    nat                = true
    security_group_ids = [yandex_vpc_security_group.example.id]
  }

  metadata = local.metadata_common
}


Файл count-vm.tf — web (count)

""
resource "yandex_compute_instance" "web" {
  count       = 2
  name        = "web-${count.index + 1}"
  platform_id = "standard-v1"
  labels      = local.common_labels

  resources {
    cores         = var.vm_resources_map["web"].cores
    memory        = var.vm_resources_map["web"].memory
    core_fraction = var.vm_resources_map["web"].core_fraction
  }

  boot_disk {
    initialize_params {
      image_id = data.yandex_compute_image.ubuntu.image_id
      type     = "network-hdd"
      size     = 10
    }
  }

  scheduling_policy { preemptible = true }

  network_interface {
    subnet_id          = yandex_vpc_subnet.develop.id
    nat                = true
    security_group_ids = [yandex_vpc_security_group.example.id]
  }

  metadata   = local.metadata_common
  depends_on = [yandex_compute_instance.db]  # web после db
}
""

Файл disk_vm.tf — 3 диска (count) + ВМ storage с dynamic secondary_disk

""
resource "yandex_compute_disk" "storage_disks" {
  count = 3
  name  = "storage-disk-${count.index + 1}"
  size  = 1
  type  = "network-hdd"
  zone  = var.default_zone
  labels = local.common_labels
}

resource "yandex_compute_instance" "storage" {
  name        = "storage"
  platform_id = "standard-v1"
  labels      = local.common_labels

  resources {
    cores         = var.vm_resources_map["web"].cores
    memory        = var.vm_resources_map["web"].memory
    core_fraction = var.vm_resources_map["web"].core_fraction
  }

  boot_disk {
    initialize_params {
      image_id = data.yandex_compute_image.ubuntu.image_id
      type     = "network-hdd"
      size     = 10
    }
  }

  dynamic "secondary_disk" {
    for_each = yandex_compute_disk.storage_disks
    content {
      disk_id = secondary_disk.value.id
    }
  }

  scheduling_policy { preemptible = true }

  network_interface {
    subnet_id          = yandex_vpc_subnet.develop.id
    nat                = true
    security_group_ids = [yandex_vpc_security_group.example.id]
  }

  metadata = local.metadata_common
}
""

Файл ansible.tftpl — шаблон инвентаря

""
[webservers]
%{ for w in web ~}
${w.name} ansible_host=${w.network_interface[0].nat_ip_address} fqdn=${w.fqdn}
%{ endfor ~}

[databases]
%{ for d in db ~}
${d.name} ansible_host=${d.network_interface[0].nat_ip_address} fqdn=${d.fqdn}
%{ endfor ~}

[storage]
%{ for s in storage ~}
${s.name} ansible_host=${s.network_interface[0].nat_ip_address} fqdn=${s.fqdn}
%{ endfor ~}
""

Файл outputs.tf — выходы без хардкода

""
output "network_id" {
  description = "VPC network id"
  value       = yandex_vpc_network.develop.id
}

output "subnet_id" {
  description = "Subnetwork id"
  value       = yandex_vpc_subnet.develop.id
}

output "web" {
  description = "web: name, external_ip, fqdn"
  value = [
    for w in yandex_compute_instance.web : {
      instance_name = w.name
      external_ip   = w.network_interface[0].nat_ip_address
      fqdn          = w.fqdn
    }
  ]
}

output "db" {
  description = "db: name, external_ip, fqdn"
  value = {
    for k, d in yandex_compute_instance.db :
    k => {
      instance_name = d.name
      external_ip   = d.network_interface[0].nat_ip_address
      fqdn          = d.fqdn
    }
  }
}

output "storage" {
  description = "storage: name, external_ip, fqdn"
  value = {
    instance_name = yandex_compute_instance.storage.name
    external_ip   = yandex_compute_instance.storage.network_interface[0].nat_ip_address
    fqdn          = yandex_compute_instance.storage.fqdn
  }
}
""

Файл terraform.tfvars — только идентификаторы

""
cloud_id  = "b1g3gq2pvbd3dnherq8j"
folder_id = "b1gio4d52dv3cbcba4ad"
# zone/cidr — из defaults в variables.tf
""
Дополнение : 

Файл providers.tf

""
terraform {
  required_version = "~> 1.12.0"

  required_providers {
    yandex = {
      source  = "yandex-cloud/yandex"
      version = "~> 0.131" # или ваша текущая
    }
  }
}

provider "yandex" {
  cloud_id  = var.cloud_id
  folder_id = var.folder_id

  # Используйте ОДИН вариант аутентификации:
  # 1) сервисный аккаунт
  service_account_key_file = var.service_account_key_file
  # 2) либо OAuth токен (если нужен)
  # token = var.yc_token
}
""

Файл variables.tf

""
# Провайдер
variable "cloud_id" {
  type        = string
  description = "YC cloud id"
}
variable "folder_id" {
  type        = string
  description = "YC folder id"
}
variable "service_account_key_file" {
  type        = string
  description = "Path to SA key JSON"
  default     = "key.json"
}
variable "yc_token" {
  type        = string
  description = "OAuth token (optional, если не используете SA)"
  default     = null
}

# Базовая сеть/подсети
variable "network_name" {
  type        = string
  default     = "netology-net"
}
variable "subnets" {
  description = "Карта подсетей: имя => { zone, v4_cidr }"
  type = map(object({
    zone    = string
    v4_cidr = string
  }))
  default = {
    public-a = { zone = "ru-central1-a", v4_cidr = "10.10.1.0/24" }
    private-b = { zone = "ru-central1-b", v4_cidr = "10.10.2.0/24" }
  }
}

# Общие метки/метаданные
variable "labels_common" {
  type        = map(string)
  default     = { env = "demo", owner = "egolokha" }
}
variable "metadata_common" {
  description = "Общая metadata для всех ВМ"
  type        = map(string)
  default = {
    "serial-port-enable" = "1"
    # ключ будет подставлен через local ниже, чтобы не хардкодить имя пользователя
  }
}
variable "ssh_user" {
  type        = string
  default     = "ubuntu"
}
variable "ssh_public_key" {
  type        = string
  description = "Публичный SSH-ключ"
}

# Каталог образа
variable "image_family" {
  type        = string
  default     = "ubuntu-22-04-lts"
}

# Security Group: декларативные правила
variable "sg_rules" {
  description = "Список SG-правил"
  type = map(object({
    direction      = string         # ingress | egress
    protocol       = string         # TCP | UDP | ANY
    port           = optional(number)
    from_port      = optional(number)
    to_port        = optional(number)
    v4_cidr_blocks = optional(list(string))
    description    = optional(string)
  }))
  default = {
    ssh_ingress = {
      direction      = "ingress"
      protocol       = "TCP"
      port           = 22
      v4_cidr_blocks = ["0.0.0.0/0"]
      description    = "SSH"
    }
    http_ingress = {
      direction      = "ingress"
      protocol       = "TCP"
      port           = 80
      v4_cidr_blocks = ["0.0.0.0/0"]
      description    = "HTTP"
    }
    all_egress = {
      direction      = "egress"
      protocol       = "ANY"
      v4_cidr_blocks = ["0.0.0.0/0"]
      description    = "Any egress"
    }
  }
}

# ВМ в виде map(object) — НИКАКОГО хардкода в ресурсах
variable "vms" {
  type = map(object({
    hostname       = string
    platform_id    = string
    zone           = string
    subnet_key     = string   # ключ в var.subnets
    nat            = bool
    cores          = number
    memory         = number
    core_fraction  = number
    boot_disk = object({
      size = number
      type = string
    })
  }))
  default = {
    web = {
      hostname       = "netology-develop-platform-web"
      platform_id    = "standard-v3"
      zone           = "ru-central1-a"
      subnet_key     = "public-a"
      nat            = true
      cores          = 2
      memory         = 2
      core_fraction  = 20
      boot_disk      = { size = 10, type = "network-hdd" }
    }
    db = {
      hostname       = "netology-develop-platform-db"
      platform_id    = "standard-v3"
      zone           = "ru-central1-b"
      subnet_key     = "private-b"
      nat            = false
      cores          = 2
      memory         = 2
      core_fraction  = 20
      boot_disk      = { size = 10, type = "network-ssd" }
    }
  }
}

# locals: собираем ssh-keys без хардкода юзера в metadata
locals {
  ssh_meta = {
    "ssh-keys" = "${var.ssh_user}:${var.ssh_public_key}"
  }
}
""


Файл security.tf

""
resource "yandex_vpc_network" "this" {
  name = var.network_name
  labels = var.labels_common
}

resource "yandex_vpc_subnet" "this" {
  for_each = var.subnets

  name           = "subnet-${each.key}"
  zone           = each.value.zone
  network_id     = yandex_vpc_network.this.id
  v4_cidr_blocks = [each.value.v4_cidr]
  labels         = var.labels_common
}

resource "yandex_vpc_security_group" "main" {
  name        = "main-sg"
  network_id  = yandex_vpc_network.this.id
  description = "Managed by Terraform"
  labels      = var.labels_common

  # динамически генерируем правила из var.sg_rules
  dynamic "ingress" {
    for_each = { for k, r in var.sg_rules : k => r if r.direction == "ingress" }
    content {
      protocol       = ingress.value.protocol
      description    = try(ingress.value.description, null)
      v4_cidr_blocks = try(ingress.value.v4_cidr_blocks, null)
      port           = try(ingress.value.port, null)
      from_port      = try(ingress.value.from_port, null)
      to_port        = try(ingress.value.to_port, null)
    }
  }

  dynamic "egress" {
    for_each = { for k, r in var.sg_rules : k => r if r.direction == "egress" }
    content {
      protocol       = egress.value.protocol
      description    = try(egress.value.description, null)
      v4_cidr_blocks = try(egress.value.v4_cidr_blocks, null)
      port           = try(egress.value.port, null)
      from_port      = try(egress.value.from_port, null)
      to_port        = try(egress.value.to_port, null)
    }
  }
}
""

Файл main.tf

`
data "yandex_compute_image" "ubuntu" {
  family = var.image_family
}

resource "yandex_compute_instance" "vm" {
  for_each = var.vms

  name        = each.key
  hostname    = each.value.hostname
  platform_id = each.value.platform_id
  zone        = each.value.zone
  labels      = var.labels_common

  resources {
    cores         = each.value.cores
    memory        = each.value.memory
    core_fraction = each.value.core_fraction
  }

  boot_disk {
    initialize_params {
      image_id = data.yandex_compute_image.ubuntu.id
      size     = each.value.boot_disk.size
      type     = each.value.boot_disk.type
    }
  }

  network_interface {
    subnet_id          = yandex_vpc_subnet.this[each.value.subnet_key].id
    nat                = each.value.nat
    security_group_ids = [ yandex_vpc_security_group.main.id ]
  }

  metadata = merge(var.metadata_common, local.ssh_meta)
}

output "instances" {
  value = {
    for k, vm in yandex_compute_instance.vm : k => {
      name        = vm.name
      hostname    = vm.hostname
      zone        = vm.zone
      fqdn        = vm.fqdn
      internal_ip = vm.network_interface[0].ip_address
      external_ip = try(vm.network_interface[0].nat_ip_address, null)
    }
  }
}
`

Файл personal.auto.tfvars_example


``
cloud_id  = "b1g***************"
folder_id = "b1f***************"

service_account_key_file = "key.json"


ssh_user       = "ubuntu"
ssh_public_key = "ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDMY... user@host"
``


Добрый день!
Добавил недостающий код в 03/src: providers.tf, variables.tf, security.tf, main.tf, а также пример personal.auto.tfvars_example.
Конфигурация без хардкода: параметры ВМ, сети, SG и metadata собираются из переменных map(object) и создаются через for_each/dynamic. Общая metadata и labels вынесены.
